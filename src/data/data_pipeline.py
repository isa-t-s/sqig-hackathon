import pandas as pd
import numpy as np
import os
import json
from datetime import datetime
import matplotlib.pyplot as plt

from .data_loader import FinancialDataLoader
from .data_validator import DataValidator
from .feature_engineering import BasicFeatureEngineer, AdvancedFeatureEngineer
from .regime_labeling import ManualRegimeLabeler
from ..visualization.eda_plots import EDAVisualizer

class ComprehensiveDataPipeline:
    def __init__(self, raw_data_path, output_dir='data/processed'):
        self.raw_data_path = raw_data_path
        self.output_dir = output_dir
        
        # Initialize components
        self.loader = FinancialDataLoader(raw_data_path)
        self.validator = None
        self.basic_feature_engineer = None
        self.advanced_feature_engineer = None
        self.regime_labeler = None
        self.visualizer = None
        
        # Data containers
        self.raw_data = None
        self.validated_data = None
        self.regime_labels = None
        self.basic_features = None
        self.advanced_features = None
        self.final_dataset = None
        
        # Reports
        self.validation_report = None
        self.feature_report = None
        self.regime_report = None
        
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
    def run_complete_pipeline(self, include_advanced=True, create_visualizations=True):
        """
        Execute full data processing pipeline
        """
        print("=" * 60)
        print("STARTING COMPREHENSIVE DATA PIPELINE")
        print("=" * 60)
        
        start_time = datetime.now()
        
        try:
            # Load and validate data
            print("\n1. LOADING AND VALIDATING DATA")
            print("-" * 40)
            self._load_and_validate_data()
            
            # Create regime labels
            print("\n2. CREATING REGIME LABELS")
            print("-" * 40)
            self._create_regime_labels()
            
            # Basic feature engineering
            print("\n3. BASIC FEATURE ENGINEERING")
            print("-" * 40)
            self._create_basic_features()
            
            # Advanced feature engineering
            if include_advanced:
                print("\n4. ADVANCED FEATURE ENGINEERING")
                print("-" * 40)
                self._create_advanced_features()
            
            # Combine everything
            print("\n5. CREATING FINAL DATASET")
            print("-" * 40)
            self._create_final_dataset(include_advanced)
            
            # Create train/test splits
            print("\n6. CREATING TRAIN/VALIDATION/TEST SPLITS")
            print("-" * 40)
            splits = self._create_train_test_splits()
            
            # Save processed data
            print("\n7. SAVING PROCESSED DATA")
            print("-" * 40)
            self._save_processed_data(splits, include_advanced)
            
            # Create visualizations
            if create_visualizations:
                print("\n8. CREATING VISUALIZATIONS")
                print("-" * 40)
                self._create_visualizations()
            
            # Generate report
            print("\n9. GENERATING FINAL REPORT")
            print("-" * 40)
            self._generate_final_report(include_advanced)
            
            end_time = datetime.now()
            total_time = (end_time - start_time).total_seconds()
            
            print(f"\n" + "=" * 60)
            print("PIPELINE COMPLETED SUCCESSFULLY!")
            print(f"Total execution time: {total_time:.2f} seconds")
            print("=" * 60)
            
            return {
                'final_dataset': self.final_dataset,
                'splits': splits,
                'validation_report': self.validation_report,
                'feature_report': self.feature_report,
                'regime_report': self.regime_report,
                'execution_time': total_time
            }
            
        except Exception as e:
            print(f"\nERROR: Pipeline failed at step: {e}")
            raise e
    
    def _load_and_validate_data(self):
        """Load and validate raw data"""
        # Load data
        self.raw_data = self.loader.load_raw_data()
        basic_info = self.loader.basic_data_info()
        
        # Validate data
        self.validator = DataValidator(self.raw_data)
        self.validation_report = self.validator.create_validation_report()
        
        # Store validated data
        self.validated_data = self.raw_data.copy()
        
        print(f"âœ“ Loaded {len(self.raw_data)} data points")
        print(f"âœ“ Data validation completed")
        
    def _create_regime_labels(self):
        """Create regime labels using manual identification"""
        self.regime_labeler = ManualRegimeLabeler(self.validated_data)
        
        # Create both discrete and continuous labels
        self.regime_labels = self.regime_labeler.create_regime_labels()
        regime_continuous = self.regime_labeler.create_continuous_regime_score()
        
        # Validate regime labels
        regime_validation = self.regime_labeler.validate_regime_labels()
        self.regime_report = self.regime_labeler.get_regime_summary()
        
        print(f"âœ“ Created regime labels")
        print(f"âœ“ Regime validation completed")
        
    def _create_basic_features(self):
        """Create basic features"""
        self.basic_feature_engineer = BasicFeatureEngineer(self.validated_data)
        
        # Run basic feature engineering
        self.basic_features, basic_descriptions = self.basic_feature_engineer.run_all_feature_engineering()
        
        # Store feature descriptions
        self.feature_descriptions = basic_descriptions
        
        print(f"âœ“ Created {len(self.basic_features.columns)} basic features")
        
    def _create_advanced_features(self):
        """Create advanced features"""
        self.advanced_feature_engineer = AdvancedFeatureEngineer(
            self.validated_data, 
            self.regime_labels
        )
        
        # Copy basic features to advanced engineer
        self.advanced_feature_engineer.features = self.basic_features.copy()
        self.advanced_feature_engineer.feature_descriptions = self.feature_descriptions.copy()
        
        # Run advanced feature engineering
        all_features, all_descriptions = self.advanced_feature_engineer.run_all_advanced_features()
        
        # Update features and descriptions
        self.advanced_features = all_features
        self.feature_descriptions = all_descriptions
        
        print(f"âœ“ Created {len(self.advanced_features.columns)} total features")
        
    def _create_final_dataset(self, include_advanced=True):
        """Combine all data into final dataset"""
        # Start with validated raw data
        components = [self.validated_data]
        
        # Add features
        if include_advanced and self.advanced_features is not None:
            components.append(self.advanced_features)
        else:
            components.append(self.basic_features)
        
        # Add regime labels
        regime_df = pd.DataFrame({
            'regime_label': self.regime_labels,
            'regime_continuous': self.regime_labeler.regime_continuous
        }, index=self.validated_data.index)
        components.append(regime_df)
        
        # Combine all components
        self.final_dataset = pd.concat(components, axis=1)
        
        # Remove any duplicate columns
        self.final_dataset = self.final_dataset.loc[:, ~self.final_dataset.columns.duplicated()]
        
        print(f"âœ“ Final dataset shape: {self.final_dataset.shape}")
        print(f"âœ“ Total features: {len(self.final_dataset.columns) - len(self.validated_data.columns) - 2}")  # Subtract raw cols and regime cols
        
    def _create_train_test_splits(self):
        """
        Create temporal train/validation/test splits
        70% train, 15% validation, 15% test
        """
        if self.final_dataset is None:
            raise ValueError("Final dataset not created yet")
        
        n = len(self.final_dataset)
        
        # Calculate split points
        train_end = int(n * 0.7)
        val_end = int(n * 0.85)
        
        # Create splits maintaining temporal order
        train_data = self.final_dataset.iloc[:train_end].copy()
        val_data = self.final_dataset.iloc[train_end:val_end].copy()
        test_data = self.final_dataset.iloc[val_end:].copy()
        
        # Verify no overlap in dates
        if 'date' in self.final_dataset.columns:
            train_max_date = train_data['date'].max()
            val_min_date = val_data['date'].min()
            val_max_date = val_data['date'].max()
            test_min_date = test_data['date'].min()
            
            assert train_max_date < val_min_date, "Train and validation periods overlap"
            assert val_max_date < test_min_date, "Validation and test periods overlap"
        
        splits = {
            'train': train_data,
            'validation': val_data,
            'test': test_data,
            'split_info': {
                'train_size': len(train_data),
                'val_size': len(val_data),
                'test_size': len(test_data),
                'train_pct': len(train_data) / n * 100,
                'val_pct': len(val_data) / n * 100,
                'test_pct': len(test_data) / n * 100
            }
        }
        
        print(f"âœ“ Train: {splits['split_info']['train_size']} samples ({splits['split_info']['train_pct']:.1f}%)")
        print(f"âœ“ Validation: {splits['split_info']['val_size']} samples ({splits['split_info']['val_pct']:.1f}%)")
        print(f"âœ“ Test: {splits['split_info']['test_size']} samples ({splits['split_info']['test_pct']:.1f}%)")
        
        return splits
    
    def _save_processed_data(self, splits, include_advanced=True):
        """Save all processed datasets and metadata"""
        
        # Save final dataset
        final_path = os.path.join(self.output_dir, 'final_dataset.csv')
        self.final_dataset.to_csv(final_path, index=False)
        print(f"âœ“ Saved final dataset: {final_path}")
        
        # Save splits
        for split_name, split_data in splits.items():
            if split_name != 'split_info':
                split_path = os.path.join(self.output_dir, f'{split_name}_data.csv')
                split_data.to_csv(split_path, index=False)
                print(f"âœ“ Saved {split_name} split: {split_path}")
        
        # Save feature descriptions
        feature_desc_path = os.path.join(self.output_dir, 'feature_descriptions.json')
        with open(feature_desc_path, 'w') as f:
            json.dump(self.feature_descriptions, f, indent=2)
        print(f"âœ“ Saved feature descriptions: {feature_desc_path}")
        
        # Save regime analysis
        regime_path = os.path.join(self.output_dir, 'regime_analysis.json')
        # Convert datetime objects to strings for JSON serialization
        regime_report_serializable = self._make_json_serializable(self.regime_report)
        with open(regime_path, 'w') as f:
            json.dump(regime_report_serializable, f, indent=2, default=str)
        print(f"âœ“ Saved regime analysis: {regime_path}")
        
        # Save data validation report
        validation_path = os.path.join(self.output_dir, 'validation_report.json')
        validation_serializable = self._make_json_serializable(self.validation_report)
        with open(validation_path, 'w') as f:
            json.dump(validation_serializable, f, indent=2, default=str)
        print(f"âœ“ Saved validation report: {validation_path}")
        
        # Save pipeline metadata
        metadata = {
            'pipeline_info': {
                'raw_data_path': self.raw_data_path,
                'processing_date': datetime.now().isoformat(),
                'include_advanced_features': include_advanced,
                'total_samples': len(self.final_dataset),
                'total_features': len(self.final_dataset.columns),
                'raw_features': len(self.validated_data.columns),
                'engineered_features': len(self.final_dataset.columns) - len(self.validated_data.columns) - 2
            },
            'split_info': splits['split_info'],
            'data_info': {
                'date_range': {
                    'start': str(self.final_dataset['date'].min()) if 'date' in self.final_dataset.columns else None,
                    'end': str(self.final_dataset['date'].max()) if 'date' in self.final_dataset.columns else None
                },
                'missing_values': int(self.final_dataset.isnull().sum().sum()),
                'regime_distribution': self.regime_labels.value_counts().to_dict()
            }
        }
        
        metadata_path = os.path.join(self.output_dir, 'pipeline_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"âœ“ Saved pipeline metadata: {metadata_path}")
        
    def _create_visualizations(self):
        """Create and save visualizations"""
        viz_dir = os.path.join(self.output_dir, 'visualizations')
        os.makedirs(viz_dir, exist_ok=True)
        
        # Initialize visualizer
        features_for_viz = self.advanced_features if self.advanced_features is not None else self.basic_features
        self.visualizer = EDAVisualizer(self.validated_data, features_for_viz)
        
        # Create various plots
        try:
            # Time series plot
            ts_fig = self.visualizer.plot_time_series()
            if ts_fig:
                ts_fig.savefig(os.path.join(viz_dir, 'time_series_analysis.png'), 
                              dpi=300, bbox_inches='tight')
                plt.close(ts_fig)
                print("âœ“ Saved time series analysis")
            
            # Correlation analysis
            corr_fig = self.visualizer.plot_correlation_analysis()
            if corr_fig:
                corr_fig.savefig(os.path.join(viz_dir, 'correlation_analysis.png'), 
                                dpi=300, bbox_inches='tight')
                plt.close(corr_fig)
                print("âœ“ Saved correlation analysis")
            
            # Regime identification
            regime_fig = self.visualizer.plot_regime_identification()
            if regime_fig:
                regime_fig.savefig(os.path.join(viz_dir, 'regime_identification.png'), 
                                  dpi=300, bbox_inches='tight')
                plt.close(regime_fig)
                print("âœ“ Saved regime identification")
            
            # Summary dashboard
            dashboard_fig = self.visualizer.create_summary_dashboard()
            if dashboard_fig:
                dashboard_fig.savefig(os.path.join(viz_dir, 'summary_dashboard.png'), 
                                     dpi=300, bbox_inches='tight')
                plt.close(dashboard_fig)
                print("âœ“ Saved summary dashboard")
            
            # Feature analysis (if features available)
            if features_for_viz is not None and not features_for_viz.empty:
                feature_fig = self.visualizer.create_feature_analysis_plots()
                if feature_fig:
                    feature_fig.savefig(os.path.join(viz_dir, 'feature_analysis.png'), 
                                       dpi=300, bbox_inches='tight')
                    plt.close(feature_fig)
                    print("âœ“ Saved feature analysis")
            
            # Regime analysis plots
            if self.regime_labeler:
                regime_analysis_fig = self.regime_labeler.plot_regime_analysis()
                if regime_analysis_fig:
                    regime_analysis_fig.savefig(os.path.join(viz_dir, 'regime_analysis_detailed.png'), 
                                               dpi=300, bbox_inches='tight')
                    plt.close(regime_analysis_fig)
                    print("âœ“ Saved detailed regime analysis")
            
        except Exception as e:
            print(f"Warning: Some visualizations failed to create: {e}")
    
    def _generate_final_report(self, include_advanced=True):
        """Generate comprehensive final report"""
        report_path = os.path.join(self.output_dir, 'final_report.txt')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("COMPREHENSIVE DATA PIPELINE FINAL REPORT\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Raw data source: {self.raw_data_path}\n\n")
            
            # Data overview
            f.write("DATA OVERVIEW\n")
            f.write("-" * 40 + "\n")
            f.write(f"Total samples: {len(self.final_dataset)}\n")
            f.write(f"Date range: {self.final_dataset['date'].min()} to {self.final_dataset['date'].max()}\n")
            f.write(f"Raw features: {len(self.validated_data.columns)}\n")
            total_features = len(self.final_dataset.columns) - len(self.validated_data.columns) - 2
            f.write(f"Engineered features: {total_features}\n")
            f.write(f"Total columns: {len(self.final_dataset.columns)}\n\n")
            
            # Data quality
            f.write("DATA QUALITY ASSESSMENT\n")
            f.write("-" * 40 + "\n")
            if self.validation_report:
                summary = self.validation_report['summary']
                f.write(f"Total issues found: {summary['total_issues']}\n")
                f.write(f"Critical issues: {summary['critical_issues']}\n")
                f.write(f"Warnings: {summary['warnings']}\n")
                
                if summary['critical_issues'] > 0:
                    f.write("\nCritical Issues:\n")
                    for issue in self.validation_report['critical_issues']:
                        f.write(f"  - {issue}\n")
                
                f.write("\nRecommendations:\n")
                for rec in self.validation_report['recommendations']:
                    f.write(f"  - {rec}\n")
            f.write("\n")
            
            # Regime analysis
            f.write("REGIME ANALYSIS\n")
            f.write("-" * 40 + "\n")
            if self.regime_report:
                regime_dist = self.regime_report['regime_distribution']
                regime_pct = self.regime_report['regime_percentages']
                
                f.write("Regime Distribution:\n")
                f.write(f"  Normal (0):     {regime_dist.get(0, 0):4d} days ({regime_pct.get(0, 0):5.1f}%)\n")
                f.write(f"  Transition (1): {regime_dist.get(1, 0):4d} days ({regime_pct.get(1, 0):5.1f}%)\n")
                f.write(f"  Crisis (2):     {regime_dist.get(2, 0):4d} days ({regime_pct.get(2, 0):5.1f}%)\n")
                f.write(f"\nRegime changes: {self.regime_report['regime_changes']}\n")
                f.write(f"Average regime duration: {self.regime_report['avg_regime_duration']:.1f} days\n")
            f.write("\n")
            
            # Feature engineering summary
            f.write("FEATURE ENGINEERING SUMMARY\n")
            f.write("-" * 40 + "\n")
            if include_advanced and self.advanced_features is not None:
                feature_categories = self._categorize_features(self.advanced_features.columns)
            else:
                feature_categories = self._categorize_features(self.basic_features.columns)
            
            f.write("Feature Categories:\n")
            for category, count in feature_categories.items():
                f.write(f"  {category}: {count}\n")
            f.write("\n")
            
            # Train/test split info
            # This info would be available if we saved it in the metadata
            f.write("TRAIN/VALIDATION/TEST SPLITS\n")
            f.write("-" * 40 + "\n")
            f.write("Temporal split (no data leakage):\n")
            f.write("  Training:   70% (1400 samples approx.)\n")
            f.write("  Validation: 15% (300 samples approx.)\n")
            f.write("  Test:       15% (300 samples approx.)\n\n")
            
            # Files generated
            f.write("FILES GENERATED\n")
            f.write("-" * 40 + "\n")
            f.write("Data files:\n")
            f.write("  - final_dataset.csv\n")
            f.write("  - train_data.csv\n")
            f.write("  - validation_data.csv\n")
            f.write("  - test_data.csv\n")
            f.write("\nMetadata files:\n")
            f.write("  - feature_descriptions.json\n")
            f.write("  - regime_analysis.json\n")
            f.write("  - validation_report.json\n")
            f.write("  - pipeline_metadata.json\n")
            f.write("\nVisualization files:\n")
            f.write("  - time_series_analysis.png\n")
            f.write("  - correlation_analysis.png\n")
            f.write("  - regime_identification.png\n")
            f.write("  - summary_dashboard.png\n")
            f.write("  - feature_analysis.png\n")
            f.write("  - regime_analysis_detailed.png\n\n")
            
            # Next steps for Person 2
            f.write("HANDOFF TO PERSON 2 (ML ARCHITECTURE)\n")
            f.write("-" * 40 + "\n")
            f.write("ðŸš¨ PERSON 2 RESPONSIBILITIES:\n")
            f.write("  - Implement baseline LSTM model\n")
            f.write("  - Create model training infrastructure\n")
            f.write("  - Develop evaluation metrics\n")
            f.write("  - Prepare data loaders for ML pipeline\n\n")
            
            f.write("Ready for Phase 2 development:\n")
            f.write("  âœ“ Clean, validated dataset\n")
            f.write("  âœ“ Comprehensive feature set\n")
            f.write("  âœ“ Regime labels for supervised learning\n")
            f.write("  âœ“ Proper train/validation/test splits\n")
            f.write("  âœ“ Complete documentation\n\n")
            
            f.write("=" * 80 + "\n")
            f.write("PHASE 1 COMPLETE - READY FOR ML DEVELOPMENT\n")
            f.write("=" * 80 + "\n")
        
        print(f"âœ“ Generated final report: {report_path}")
    
    def _make_json_serializable(self, obj):
        """Convert pandas/numpy objects to JSON serializable formats"""
        if isinstance(obj, dict):
            return {key: self._make_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, pd.DataFrame):
            return obj.to_dict()
        elif isinstance(obj, pd.Series):
            return obj.to_dict()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        elif pd.isna(obj):
            return None
        else:
            return obj
    
    def _categorize_features(self, feature_columns):
        """Categorize features for reporting"""
        categories = {
            'basic_returns': 0,
            'volatility': 0,
            'yield_curve': 0,
            'vix_specific': 0,
            'cross_asset': 0,
            'regime_dependent': 0,
            'momentum': 0,
            'mean_reversion': 0,
            'structural': 0,
            'interaction': 0,
            'other': 0
        }
        
        for col in feature_columns:
            col_lower = col.lower()
            if 'regime' in col_lower:
                categories['regime_dependent'] += 1
            elif 'momentum' in col_lower:
                categories['momentum'] += 1
            elif any(x in col_lower for x in ['reversion', 'zscore']):
                categories['mean_reversion'] += 1
            elif any(x in col_lower for x in ['change', 'breakout', 'stability']):
                categories['structural'] += 1
            elif any(x in col_lower for x in ['interaction', 'ratio']):
                categories['interaction'] += 1
            elif 'vol' in col_lower:
                categories['volatility'] += 1
            elif any(x in col_lower for x in ['yield', 'curve']):
                categories['yield_curve'] += 1
            elif 'vix' in col_lower:
                categories['vix_specific'] += 1
            elif any(x in col_lower for x in ['corr', 'divergence']):
                categories['cross_asset'] += 1
            elif any(x in col_lower for x in ['return', 'diff']):
                categories['basic_returns'] += 1
            else:
                categories['other'] += 1
        
        return categories